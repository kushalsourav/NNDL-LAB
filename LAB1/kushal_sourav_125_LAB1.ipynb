{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">NNDL LAB</p>\n",
    "LAB - 1\n",
    "\n",
    "<p>Name: Kushal Sourav B</p>\n",
    "<p>Regno: 2347125</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, learning_rate=0.1, epochs=10):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    # ReLU activation function\n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    # Derivative of ReLU \n",
    "    def relu_derivative(self, z):\n",
    "        return np.where(z > 0, 1, 0)\n",
    "\n",
    "    #Train the perceptron using gradient descent\n",
    "    def train(self, X, y, random_weights=True):\n",
    "        n_samples, n_features = X.shape\n",
    "    \n",
    "        if random_weights:\n",
    "            self.weights = np.random.rand(n_features)  \n",
    "        else:\n",
    "            self.weights = np.array([0.5, 0.5])  \n",
    "        self.bias = 0.0\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            for idx, x_i in enumerate(X):\n",
    "             \n",
    "                linear_output = np.dot(x_i, self.weights) + self.bias\n",
    "                predicted = self.relu(linear_output)\n",
    "                error = y[idx] - predicted\n",
    "                gradient = error * self.relu_derivative(predicted)\n",
    "                self.weights += self.learning_rate * gradient * x_i\n",
    "                self.bias += self.learning_rate * gradient\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_output = np.dot(X, self.weights) + self.bias\n",
    "        return self.relu(linear_output)\n",
    "\n",
    "perceptron = Perceptron(learning_rate=0.1, epochs=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AND\n",
    "<p>Truth Table for AND Gate:</p>\n",
    "\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Input 1</th>\n",
    "      <th>Input 2</th>\n",
    "      <th>Output</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>0</td>\n",
    "      <td>1</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>1</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>1</td>\n",
    "      <td>1</td>\n",
    "      <td>1</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Random Weights:\n",
      "Testing the Model with Random Weights:\n",
      "Input: [0 0] - Predicted: 0.0 - Actual: 0\n",
      "Input: [0 1] - Predicted: 0.05 - Actual: 0\n",
      "Input: [1 0] - Predicted: 0.05 - Actual: 0\n",
      "Input: [1 1] - Predicted: 0.95 - Actual: 1\n",
      "\n",
      "Training with Defined Weights (0.5, 0.5):\n",
      "Testing the Model with Defined Weights:\n",
      "Input: [0 0] - Predicted: 0.0 - Actual: 0\n",
      "Input: [0 1] - Predicted: 0.04 - Actual: 0\n",
      "Input: [1 0] - Predicted: 0.04 - Actual: 0\n",
      "Input: [1 1] - Predicted: 0.96 - Actual: 1\n"
     ]
    }
   ],
   "source": [
    "andX = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) \n",
    "andY = np.array([0, 0, 0, 1])  \n",
    "\n",
    "\n",
    "print(\"Training with Random Weights:\")\n",
    "perceptron.train(andX, andY, random_weights=True)\n",
    "\n",
    "print(\"Testing the Model with Random Weights:\")\n",
    "for x, target in zip(andX,andY):\n",
    "    output = perceptron.predict(x)\n",
    "    print(f\"Input: {x} - Predicted: {round(output, 2)} - Actual: {target}\")\n",
    "\n",
    "print(\"\\nTraining with Defined Weights (0.5, 0.5):\")\n",
    "perceptron.train(andX,andY, random_weights=False)\n",
    "\n",
    "\n",
    "print(\"Testing the Model with Defined Weights:\")\n",
    "for x, target in zip(andX, andY):\n",
    "    output = perceptron.predict(x)\n",
    "    print(f\"Input: {x} - Predicted: {round(output, 2)} - Actual: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions - AND Gate\n",
    "\n",
    "##### How do the weights and bias values change during training for the AND gate?\n",
    "* Initially, the weights and bias are random. As the perceptron encounters training errors, it updates them based on the difference between predicted and actual output (the error), using the learning rate to control the step size.\n",
    "##### Can the perceptron successfully learn the AND logic with a linear decision boundary?\n",
    "* Yes, the AND gate is linearly separable, so a Single Layer Perceptron can successfully learn to classify it with a linear decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OR Gate\n",
    "<p> Truth Table for OR Gate </p>\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Input 1</th>\n",
    "      <th>Input 2</th>\n",
    "      <th>Output</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>0</td>\n",
    "      <td>1</td>\n",
    "      <td>1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>1</td>\n",
    "      <td>0</td>\n",
    "      <td>1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>1</td>\n",
    "      <td>1</td>\n",
    "      <td>1</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0 0] - Predicted: 0.28 - Actual: 0\n",
      "Input: [0 1] - Predicted: 0.75 - Actual: 1\n",
      "Input: [1 0] - Predicted: 0.72 - Actual: 1\n",
      "Input: [1 1] - Predicted: 1.19 - Actual: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "orX = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "orY = np.array([0, 1, 1, 1]) \n",
    "\n",
    "perceptron_or = Perceptron(learning_rate=0.1, epochs=8000)\n",
    "perceptron_or.train(orX, orY, random_weights=True)\n",
    "\n",
    "for x, target in zip(orX, orY):\n",
    "    output = perceptron_or.predict(x)\n",
    "    print(f\"Input: {x} - Predicted: {round(output, 2)} - Actual: {target}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions - OR Gate\n",
    "\n",
    "##### What changes in the perceptron's weights are necessary to represent the OR gate logic?\n",
    "* The weights will adjust to reflect that as long as one of the inputs is 1, the output should be 1. Thus, the weights tend to be positive enough to push the linear combination above the activation threshold in the presence of 1s.\n",
    "\n",
    "##### How does the linear decision boundary look for the OR gate classification?\n",
    "* The decision boundary separates the inputs (0,1), (1,0), and (1,1) from (0,0), representing a linear decision surface where any non-zero input leads to a positive output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AND-NOT Gate\n",
    "<p> Truth Table for AND-NOT Gate</p>\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Input 1</th>\n",
    "      <th>Input 2</th>\n",
    "      <th>Output</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>0</td>\n",
    "      <td>1</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>1</td>\n",
    "      <td>0</td>\n",
    "      <td>1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>1</td>\n",
    "      <td>1</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0 0] - Predicted: 0 - Actual: 0\n",
      "Input: [0 1] - Predicted: 0 - Actual: 0\n",
      "Input: [1 0] - Predicted: 1 - Actual: 1\n",
      "Input: [1 1] - Predicted: 0 - Actual: 0\n",
      "Classification Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_andnot = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  \n",
    "y_andnot = np.array([0, 0, 1, 0])  \n",
    "\n",
    "perceptron_andnot = Perceptron(learning_rate=0.1, epochs=100)\n",
    "perceptron_andnot.train(X_andnot, y_andnot, random_weights=True)\n",
    "\n",
    "correct_predictions = 0  \n",
    "\n",
    "\n",
    "for x, target in zip(X_andnot, y_andnot):\n",
    "    output = perceptron_andnot.predict(x)\n",
    "    predicted = 1 if round(output) >= 0.5 else 0  \n",
    "    print(f\"Input: {x} - Predicted: {predicted} - Actual: {target}\")\n",
    "    \n",
    "    if predicted == target:\n",
    "        correct_predictions += 1\n",
    "        \n",
    "accuracy = correct_predictions / len(y_andnot) * 100\n",
    "print(f\"Classification Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions - AND-NOT\n",
    "\n",
    "##### What is the perceptron's weight configuration after training for the AND-NOT gate?\n",
    "* The weights will adjust such that the perceptron responds only when the first input is 1 and the second input is 0, reflecting the AND-NOT condition.\n",
    "##### How does the perceptron handle cases where both inputs are 1 or 0?\n",
    "* The perceptron outputs 0 for both these cases, as required by the AND-NOT logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XOR Gate\n",
    "\n",
    "<p> Truth Table for XOR Gate </p>\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Input 1</th>\n",
    "      <th>Input 2</th>\n",
    "      <th>Output</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>0</td>\n",
    "      <td>1</td>\n",
    "      <td>1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>1</td>\n",
    "      <td>0</td>\n",
    "      <td>1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>1</td>\n",
    "      <td>1</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0 0] - Predicted: 0.55 - Actual: 0\n",
      "Input: [0 1] - Predicted: 0.5 - Actual: 1\n",
      "Input: [1 0] - Predicted: 0.44 - Actual: 1\n",
      "Input: [1 1] - Predicted: 0.39 - Actual: 0\n"
     ]
    }
   ],
   "source": [
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  \n",
    "y_xor = np.array([0, 1, 1, 0])  \n",
    "\n",
    "perceptron_xor = Perceptron(learning_rate=0.1, epochs=100)\n",
    "perceptron_xor.train(X_xor, y_xor, random_weights=True)\n",
    "\n",
    "for x, target in zip(X_xor, y_xor):\n",
    "    output = perceptron_xor.predict(x)\n",
    "    print(f\"Input: {x} - Predicted: {round(output, 2)} - Actual: {target}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe and discuss the perceptron's performance in this scenario.\n",
    "\n",
    "The perceptron will struggle with the XOR gate because XOR is not linearly separable (no single straight line can separate the classes). A single-layer perceptron can only solve linearly separable problems like AND, OR, and AND-NOT. To solve XOR, you'd need a more complex model, such as a multi-layer perceptron (MLP) with non-linear activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions - XOR Gate\n",
    "\n",
    "##### Why does the Single Layer Perceptron struggle to classify the XOR gate?\n",
    "* XOR is not linearly separable, meaning it cannot be correctly classified by a Single Layer Perceptron because its decision boundary is non-linear.\n",
    "\n",
    "##### What modifications can be made to the neural network model to handle the XOR gate correctly?\n",
    "* A Multi-Layer Perceptron (MLP) with at least one hidden layer can successfully classify XOR by learning non-linear decision boundarie"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
